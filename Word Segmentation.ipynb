{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6cfbb5",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d79c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\n",
      "asttokens==2.2.1\n",
      "astunparse==1.6.3\n",
      "attrs==22.2.0\n",
      "backcall==0.2.0\n",
      "cachetools==5.3.0\n",
      "certifi==2022.12.7\n",
      "charset-normalizer==3.0.1\n",
      "click==7.1.2\n",
      "colorama==0.4.6\n",
      "comm==0.1.2\n",
      "debugpy==1.6.6\n",
      "decorator==5.1.1\n",
      "dill==0.3.6\n",
      "executing==1.2.0\n",
      "flatbuffers==23.1.21\n",
      "future==0.18.3\n",
      "gast==0.4.0\n",
      "google-auth==2.16.0\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-pasta==0.2.0\n",
      "googleapis-common-protos==1.58.0\n",
      "grpcio==1.51.1\n",
      "h5py==3.8.0\n",
      "idna==3.4\n",
      "ipykernel==6.20.2\n",
      "ipython==8.9.0\n",
      "jax==0.4.7\n",
      "jedi==0.18.2\n",
      "jupyter_client==8.0.1\n",
      "jupyter_core==5.1.5\n",
      "keras==2.12.0\n",
      "libclang==15.0.6.1\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.2\n",
      "matplotlib-inline==0.1.6\n",
      "ml-dtypes==0.0.4\n",
      "nest-asyncio==1.5.6\n",
      "numpy==1.23.5\n",
      "oauthlib==3.2.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.0\n",
      "pandas==1.5.3\n",
      "parso==0.8.3\n",
      "pickleshare==0.7.5\n",
      "platformdirs==2.6.2\n",
      "promise==2.3\n",
      "prompt-toolkit==3.0.36\n",
      "protobuf==3.20.3\n",
      "psutil==5.9.4\n",
      "pure-eval==0.2.2\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "Pygments==2.14.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.7.1\n",
      "pywin32==305\n",
      "pyzmq==25.0.0\n",
      "regex==2022.10.31\n",
      "requests==2.28.2\n",
      "requests-oauthlib==1.3.1\n",
      "rsa==4.9\n",
      "scipy==1.10.1\n",
      "six==1.16.0\n",
      "stack-data==0.6.2\n",
      "tensorboard==2.12.0\n",
      "tensorboard-data-server==0.7.0\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow-addons==0.19.0\n",
      "tensorflow-datasets==3.2.1\n",
      "tensorflow-estimator==2.12.0\n",
      "tensorflow-io-gcs-filesystem==0.30.0\n",
      "tensorflow-metadata==1.12.0\n",
      "termcolor==2.2.0\n",
      "tornado==6.2\n",
      "tqdm==4.64.1\n",
      "traitlets==5.8.1\n",
      "typeguard==2.13.3\n",
      "typing_extensions==4.4.0\n",
      "urllib3==1.26.14\n",
      "wcwidth==0.2.6\n",
      "Werkzeug==2.2.2\n",
      "wrapt==1.14.1\n"
     ]
    }
   ],
   "source": [
    "# for installing UrduHack\n",
    "# !pip install urduhack[tf]\n",
    "# for installing nltk\n",
    "# !pip install nltk\n",
    "import urduhack\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd32767",
   "metadata": {},
   "source": [
    "# Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d6015",
   "metadata": {},
   "source": [
    "Generic Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc78b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Dictionary.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3036802",
   "metadata": {},
   "source": [
    "Urdu Date Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7489d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dates.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary1=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary1.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60192f",
   "metadata": {},
   "source": [
    "Urdu Bigram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1dc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_words.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary2=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary2.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0175983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ef408",
   "metadata": {},
   "source": [
    "Urdu Places Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54575174",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('locations.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary3=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary3.append(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ed986",
   "metadata": {},
   "source": [
    "Urdu organization words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ed917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('organizations.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary4=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary4.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5eeaa",
   "metadata": {},
   "source": [
    "Urdu Random Persons Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943f7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('persons.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "    \n",
    "Dictionary5=[]\n",
    "for i in passage:\n",
    "    for j in i:\n",
    "        Dictionary5.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bccb60",
   "metadata": {},
   "source": [
    "Combined Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e127c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154781\n",
      "55\n",
      "84\n",
      "2426\n",
      "1064\n",
      "3348\n"
     ]
    }
   ],
   "source": [
    "print(len(Dictionary))\n",
    "print(len(Dictionary1))\n",
    "print(len(Dictionary2))\n",
    "print(len(Dictionary3))\n",
    "print(len(Dictionary4))\n",
    "print(len(Dictionary5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b44f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary.extend(Dictionary1)\n",
    "Dictionary.extend(Dictionary2)\n",
    "Dictionary.extend(Dictionary3)\n",
    "Dictionary.extend(Dictionary4)\n",
    "Dictionary.extend(Dictionary5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58098ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161758"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7943f3",
   "metadata": {},
   "source": [
    "# Preprocessing on dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82425d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = lambda x: x.replace('_', ' ')\n",
    "updated_list = list(map(converter, Dictionary)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "718ab568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161758"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4606f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = sorted(updated_list, key=len, reverse=True)\n",
    "\n",
    "dic = np.array(Dictionary)\n",
    "dic, counts = np.unique(dic, return_counts=True)\n",
    "\n",
    "def merge(list1, list2):\n",
    "     \n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list\n",
    "\n",
    "tup=merge(dic, counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c452e140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158396"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2553a5",
   "metadata": {},
   "source": [
    "# Word Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4cae4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_test.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    text3 = csv.reader(f)\n",
    "    unsegmented_words = list(text3)\n",
    "# unsegmented_words[0][0]\n",
    "\n",
    "\n",
    "with open('word-segmented.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    text4 = csv.reader(f)\n",
    "    words_test_case = list(text4)\n",
    "    \n",
    "# with open('segmented.txt', 'rt' , encoding=\"utf8\") as file:\n",
    "#     test_of_words = file.read(f)\n",
    "#     words = contents.split()\n",
    "#     file.close()\n",
    "# words = [word.strip() for word in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e93f5",
   "metadata": {},
   "source": [
    "# Word Segmentation function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad90861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_parser(var1):\n",
    "    test_list=[]\n",
    "    for i in var1:\n",
    "        test_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb4f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_exist=[]\n",
    "temp=\"\"\n",
    "size=max(len(j) for j in dic)\n",
    "def word_segmentation(size,dic,var1):\n",
    "    for k in reversed(range(size)):\n",
    "        dummy=var1[:k]\n",
    "        if dummy in dic:\n",
    "            if len(dummy)== k :\n",
    "                word_exist.append(dummy)\n",
    "                word_len=len(dummy)  \n",
    "                dummy=\"\"\n",
    "                var1=var1[word_len:]\n",
    "                break\n",
    "    return word_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08962a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Words:\n",
      "\n",
      "['تجربہ', 'آفس', 'غورطلب', 'توشہ', 'تفصیلات', 'جج', 'یاد', 'سابق', 'مفتاح', 'سابق', 'وزیرداخلہ', 'اے', 'قبل', 'ایازصادق']\n",
      "\n",
      "\n",
      "Segmented Words String:\n",
      "\n",
      "['تجربہ آفس غورطلب توشہ تفصیلات جج یاد سابق مفتاح سابق وزیرداخلہ اے قبل ایازصادق']\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for i in range(len(unsegmented_words)):\n",
    "    sentence=unsegmented_words[i][0]\n",
    "    res=word_segmentation(25, dic,sentence)\n",
    "    \n",
    "print(\"Segmented Words:\\n\")\n",
    "print(res)\n",
    "words.append(\" \".join(res))\n",
    "print(\"\\n\")\n",
    "print(\"Segmented Words String:\\n\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e104ce",
   "metadata": {},
   "source": [
    "# Word function test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8fabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=words_test_case[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0327bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(res,test):\n",
    "    yes=0\n",
    "    no=0\n",
    "    for i,val in enumerate(res):\n",
    "        if val in test[i]:\n",
    "            yes=yes+1\n",
    "        else:\n",
    "            no=no+1\n",
    "    acc=(yes/len(res))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9720b5",
   "metadata": {},
   "source": [
    "# Word Segmentation Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11b74395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_length_match(var, size,dic):\n",
    "    word_lis = []\n",
    "    while var:\n",
    "        for i in reversed(range(1,size)):\n",
    "            if len(var) >= i :\n",
    "                if var[-i:] in dic:\n",
    "                    word_lis.append(var[-i:])\n",
    "                    var = var[:-i]\n",
    "                    break\n",
    "        else:\n",
    "            word_lis.append(var[-1])\n",
    "            var = var[:-1]\n",
    "    return word_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1da55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "size=max(len(j) for j in dic)\n",
    "for i in range(len(unsegmented_words)):\n",
    "    sentence=unsegmented_words[i][0]\n",
    "    res=maximum_length_match(sentence, 25,dic)\n",
    "    res.reverse()\n",
    "    output.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a77d7",
   "metadata": {},
   "source": [
    "# Word Function Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4e5dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word-segmented.txt', 'rt' , encoding=\"utf8\") as file:\n",
    "    contents = file.read()\n",
    "    words = contents.split()\n",
    "    file.close()\n",
    "acc_words = [word.strip() for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ccb1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_words=acc_words[:len(output[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f73e9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=evaluate(output[0],acc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691b661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Segmentation output:\n",
      "['تجربہ کار ہندوستانی آف سپنر روی چند رن ای شون نے آئن دہ ایشیاء کپ 2 0 2 3 ء کی غیر یقینی قسمت پر اپنی رائے کااظہار کیاہے جو پاکستان میں ہونے جارہاہے اپنے یوٹیوب چینل پر بات کرتے ہوئے روی چند رن ای شون نے کہاکہ اگر پڑوسی ملک بھارت ایشیا کپ 2 0 2 3 ء میں شرکت کرنا چاہتاہے تو مقام تبدیل کردینا چاہیے']\n",
      "\n",
      "\n",
      "provided test case:\n",
      "['تجربہ کار ہندوستانی آف سپنر روی چندرن ایشون نے آئندہ ایشیاء کپ 2023ء کی غیر یقینی قسمت پر اپنی رائے کا اظہار کیا ہے، جو پاکستان میں ہونے جا رہا ہے۔ اپنے یوٹیوب چینل پر بات کرتے ہوئے روی چندرن ایشون نے کہا کہ اگر پڑوسی ملک بھارت ایشیا کپ 2023ء میں شرکت کرنا چاہتا ہے تو مقام تبدیل کر دینا چاہیے۔']\n"
     ]
    }
   ],
   "source": [
    "output_string=[]\n",
    "output_string.append(\" \".join(output[0]))\n",
    "print(\"Word Segmentation output:\")\n",
    "print(output_string)\n",
    "print(\"\\n\")\n",
    "print(\"provided test case:\")\n",
    "print(words_test_case[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2b2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
